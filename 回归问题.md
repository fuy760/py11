# 一、回归问题

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 加载加利福尼亚房价数据集
housing = fetch_california_housing()
X = housing.data
y = housing.target

# 数据集划分为训练集、验证集和测试集
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

# 数据标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)

# 转换为PyTorch张量
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)
X_valid_tensor = torch.FloatTensor(X_valid)
y_valid_tensor = torch.FloatTensor(y_valid).reshape(-1, 1)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)

# 创建数据加载器
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=64)

# 定义神经网络模型
class RegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(RegressionModel, self).__init__()
        self.layer1 = nn.Linear(input_dim, 30)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(30, 1)
        
    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        return x

# 初始化模型
input_dim = X_train.shape[1]
model = RegressionModel(input_dim)

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters())

# 训练模型
num_epochs = 50
patience = 5
best_val_loss = float('inf')
counter = 0
train_losses = []
val_losses = []
train_maes = []
val_maes = []

for epoch in range(num_epochs):
    # 训练阶段
    model.train()
    train_loss = 0
    train_mae = 0
    for X_batch, y_batch in train_loader:
        # 前向传播
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch)
        
        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        train_mae += torch.mean(torch.abs(y_pred - y_batch)).item()
    
    train_loss /= len(train_loader)
    train_mae /= len(train_loader)
    train_losses.append(train_loss)
    train_maes.append(train_mae)
    
    # 验证阶段
    model.eval()
    val_loss = 0
    val_mae = 0
    with torch.no_grad():
        for X_batch, y_batch in valid_loader:
            y_pred = model(X_batch)
            val_loss += criterion(y_pred, y_batch).item()
            val_mae += torch.mean(torch.abs(y_pred - y_batch)).item()
    
    val_loss /= len(valid_loader)
    val_mae /= len(valid_loader)
    val_losses.append(val_loss)
    val_maes.append(val_mae)
    
    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae:.4f}, Val MAE: {val_mae:.4f}')
    
    # 早停
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print(f'Early stopping at epoch {epoch+1}')
            break

# 评估模型
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor)
    test_mse = criterion(y_pred, y_test_tensor).item()
    test_rmse = np.sqrt(test_mse)
    test_mae = torch.mean(torch.abs(y_pred - y_test_tensor)).item()

print(f"测试集MSE: {test_mse:.4f}")
print(f"测试集RMSE: {test_rmse:.4f}")
print(f"测试集MAE: {test_mae:.4f}")

# 绘制学习曲线
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses)
plt.plot(val_losses)
plt.title('模型损失')
plt.ylabel('损失')
plt.xlabel('Epoch')
plt.legend(['训练集', '验证集'], loc='upper right')

plt.subplot(1, 2, 2)
plt.plot(train_maes)
plt.plot(val_maes)
plt.title('平均绝对误差')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['训练集', '验证集'], loc='upper right')
plt.tight_layout()
plt.show()
```

# 二、w权重

![image-20250731132206334](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731132206334.png)



![image-20250731132213682](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731132213682.png)



![image-20250731132224112](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731132224112.png)



![image-20250731132919826](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731132919826.png)

![image-20250731132932190](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731132932190.png)



![image-20250731133134900](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731133134900.png)

![image-20250731133754284](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731133754284.png)

![image-20250731153808790](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731153808790.png)



![image-20250731153816754](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250731153816754.png)

# 三、反向传播与梯度下降

![image-20250805153057034](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250805153057034.png)

# 四、激活函数哦

## 1、sigmoid

主要解决二分类问题，输出层一个神经元

![image-20250805161135820](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250805161135820.png)

## 2、ReLU

![image-20250805161155785](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250805161155785.png)

![image-20250805162705191](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250805162705191.png)

![image-20250805162714618](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250805162714618.png)



# 五、Dropout

![image-20250805163804552](C:\Users\NYZ\AppData\Roaming\Typora\typora-user-images\image-20250805163804552.png)

我们在训练模型时，让网络中某些“神经元罢工”，让模型**不能依赖某一小撮“聪明”的神经元来完成任务**，而是逼迫整个网络都学到有用的特征。这样一来，模型在面对新的数据时，就不容易过拟合训练集。



# 六、wide&deep
